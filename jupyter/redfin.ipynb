{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Load Libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "\n",
    "# data structures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# others\n",
    "import sys, os, re, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>UDF</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scrapping time\n",
    "def get_scrapping_time() -> str:\n",
    "    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pages to iterate\n",
    "def get_pages(browser: WebDriver) -> list:\n",
    "    pages = []\n",
    "    for button in browser.find_elements(By.XPATH, \"//span[@class='ButtonLabel']\"):\n",
    "        try:\n",
    "            float(button.text.strip())\n",
    "            pages.append(button)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract table from each page\n",
    "def extract_table(raw_html: str, table: dict) -> None:\n",
    "    soup = BeautifulSoup(raw_html)\n",
    "    rows = soup.find_all('table')[1].find_all('tr')\n",
    "    ## we do not use the last row\n",
    "    for row in rows[:-1]:\n",
    "        data = [td.text.strip() for td in row.find_all('td') if td.text.strip() != '']\n",
    "        ###\n",
    "        keys = list(table.keys())\n",
    "        for i in range(len(data)):\n",
    "            table[keys[i]].append(data[i])\n",
    "        ###\n",
    "        table['scrapping_date'] = get_scrapping_time()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the button for switching to table format\n",
    "def get_table_button(browser):\n",
    "    try:\n",
    "        ## box\n",
    "        button = browser.find_element(By.XPATH, '/html/body/div[1]/div[8]/div[1]/div[2]/div[1]/div[2]/div/div/div/div[2]/div[2]/ul/li[2]/span')\n",
    "    except:\n",
    "        ## drop-down\n",
    "        button = browser.find_element(By.XPATH, '/html/body/div[1]/div[8]/div[1]/div[3]/div[1]/div[1]/div/div/div/div/div[2]/div/div[2]/div[2]/div/div/div/label/div/select/option[2]')\n",
    "\n",
    "    return button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping - Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "## browser\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to table format\n",
    "browser.get('https://www.redfin.com/city/29470/IL/Chicago')\n",
    "button = get_table_button(browser)\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pages \n",
    "pages = get_pages(browser)\n",
    "## iterate each page to scrape\n",
    "table = {'address': [], 'location': [], 'price': [], 'beds': [], \n",
    "         'baths': [], 'sq.ft': [], '$/sq.ft': [], 'on_redfin': [], \n",
    "         'scrapping_date': []}\n",
    "for page in pages:\n",
    "    page.click()\n",
    "    extract_table(browser.page_source, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scrapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
